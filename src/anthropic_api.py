"""
Anthropic API ÂÖºÂÆπÂ±Ç
Êèê‰æõÂÆåÊï¥ÁöÑ Anthropic Messages API ÊîØÊåÅÔºåÂåÖÊã¨Êï∞ÊçÆÊ®°Âûã„ÄÅËΩ¨Êç¢Âô®ÂíåÊµÅÂºèÂìçÂ∫î
"""

import asyncio
import time
import uuid
import json
from typing import List, Dict, Optional, Any, Union, AsyncGenerator, Literal
from dataclasses import dataclass
from enum import Enum
from pydantic import BaseModel, Field, field_validator
import logging

logger = logging.getLogger(__name__)

# ANSI color codes for beautiful logging
class Colors:
    CYAN = "\033[96m"
    BLUE = "\033[94m"
    GREEN = "\033[92m"
    YELLOW = "\033[93m"
    RED = "\033[91m"
    MAGENTA = "\033[95m"
    RESET = "\033[0m"
    BOLD = "\033[1m"

def log_request_beautifully(method: str, path: str, anthropic_model: str, gemini_model: str, 
                          num_messages: int, num_tools: int, status_code: int = 200):
    """Log requests in a beautiful format showing model mapping"""
    claude_display = f"{Colors.CYAN}{anthropic_model}{Colors.RESET}"
    gemini_display = f"{Colors.GREEN}{gemini_model}{Colors.RESET}"
    tools_str = f"{Colors.MAGENTA}{num_tools} tools{Colors.RESET}" if num_tools > 0 else ""
    messages_str = f"{Colors.BLUE}{num_messages} messages{Colors.RESET}"
    status_str = f"{Colors.GREEN}‚úì {status_code} OK{Colors.RESET}" if status_code == 200 else f"{Colors.RED}‚úó {status_code}{Colors.RESET}"
    
    endpoint = path.split("?")[0]
    log_line = f"{Colors.BOLD}{method} {endpoint}{Colors.RESET} {status_str}"
    model_line = f"{claude_display} ‚Üí {gemini_display} {tools_str} {messages_str}"
    
    print(log_line)
    print(model_line)

# ========== Anthropic API Êï∞ÊçÆÊ®°Âûã ==========

class ContentBlockText(BaseModel):
    type: Literal["text"] = "text"
    text: str

class ContentBlockImage(BaseModel):
    type: Literal["image"] = "image"
    source: Dict[str, Any]

class ContentBlockToolUse(BaseModel):
    type: Literal["tool_use"] = "tool_use"
    id: str
    name: str
    input: Dict[str, Any]

class ContentBlockToolResult(BaseModel):
    type: Literal["tool_result"] = "tool_result"
    tool_use_id: str
    content: Union[str, List[Dict[str, Any]], Dict[str, Any], List[Any], Any]

class SystemContent(BaseModel):
    type: Literal["text"] = "text"
    text: str

class Message(BaseModel):
    role: Literal["user", "assistant"] 
    content: Union[str, List[Union[ContentBlockText, ContentBlockImage, ContentBlockToolUse, ContentBlockToolResult]]]

class Tool(BaseModel):
    name: str
    description: Optional[str] = None
    input_schema: Dict[str, Any]

class ThinkingConfig(BaseModel):
    enabled: bool

class MessagesRequest(BaseModel):
    model: str
    max_tokens: int
    messages: List[Message]
    system: Optional[Union[str, List[SystemContent]]] = None
    stop_sequences: Optional[List[str]] = None
    stream: Optional[bool] = False
    temperature: Optional[float] = 1.0
    top_p: Optional[float] = None
    top_k: Optional[int] = None
    metadata: Optional[Dict[str, Any]] = None
    tools: Optional[List[Tool]] = None
    tool_choice: Optional[Dict[str, Any]] = None
    thinking: Optional[ThinkingConfig] = None

class TokenCountRequest(BaseModel):
    model: str
    messages: List[Message]
    system: Optional[Union[str, List[SystemContent]]] = None
    tools: Optional[List[Tool]] = None
    thinking: Optional[ThinkingConfig] = None
    tool_choice: Optional[Dict[str, Any]] = None

class TokenCountResponse(BaseModel):
    input_tokens: int

class Usage(BaseModel):
    input_tokens: int
    output_tokens: int
    cache_creation_input_tokens: int = 0
    cache_read_input_tokens: int = 0

class MessagesResponse(BaseModel):
    id: str
    model: str
    role: Literal["assistant"] = "assistant"
    content: List[Union[ContentBlockText, ContentBlockToolUse]]
    type: Literal["message"] = "message"
    stop_reason: Optional[Literal["end_turn", "max_tokens", "stop_sequence", "tool_use"]] = None
    stop_sequence: Optional[str] = None
    usage: Usage

# ========== ËΩ¨Êç¢Âô®Á±ª ==========

class AnthropicToGeminiConverter:
    """Â∞Ü Anthropic API ËØ∑Ê±ÇËΩ¨Êç¢‰∏∫ Gemini Ê†ºÂºè"""
    
    def __init__(self):
        self.model_mapping = {
            "claude-3-5-sonnet": "gemini-2.5-pro",
            "claude-3-5-haiku": "gemini-2.5-flash",
            "claude-3-opus": "gemini-2.5-pro",
            "claude-3-sonnet": "gemini-2.5-pro",
            "claude-3-haiku": "gemini-2.5-flash",
            # Ê∑ªÂä†Êñ∞ÁöÑ„ÄÅÂèØËÉΩÁî±ÂÆ¢Êà∑Á´ØÁîüÊàêÁöÑÊ®°ÂûãÂêçÁß∞ÂâçÁºÄ
            "claude-sonnet-4": "gemini-2.5-pro",
        }
    
    def convert_model(self, anthropic_model: str) -> str:
        """Â∞Ü Anthropic Ê®°ÂûãÂêçËΩ¨Êç¢‰∏∫ Gemini Ê®°ÂûãÂêçÔºåÊîØÊåÅÂâçÁºÄÂåπÈÖç‰ª•ÂÖºÂÆπÊó•ÊúüÂêéÁºÄ"""
        # ‰ºòÂÖàËøõË°åÁ≤æÁ°ÆÂåπÈÖç
        if anthropic_model in self.model_mapping:
            gemini_model = self.model_mapping[anthropic_model]
            logger.debug(f"üìã MODEL MAPPING (Exact): {anthropic_model} ‚Üí {gemini_model}")
            return gemini_model
        
        # Â¶ÇÊûúÁ≤æÁ°ÆÂåπÈÖçÂ§±Ë¥•ÔºåÂàôÊåâÈïøÂ∫¶ÈôçÂ∫èÂ∞ùËØïÂâçÁºÄÂåπÈÖç
        # ËøôÂèØ‰ª•Á°Æ‰øù "claude-3-5-sonnet" ‰ºòÂÖà‰∫é "claude-3-sonnet" Ë¢´ÂåπÈÖç
        sorted_keys = sorted(self.model_mapping.keys(), key=len, reverse=True)
        for key in sorted_keys:
            if anthropic_model.startswith(key):
                gemini_model = self.model_mapping[key]
                logger.debug(f"üìã MODEL MAPPING (Prefix): {anthropic_model} ‚Üí {gemini_model}")
                return gemini_model

        # Â¶ÇÊûúÊâÄÊúâÂåπÈÖçÈÉΩÂ§±Ë¥•ÔºåÂõûÈÄÄÂà∞ÈªòËÆ§ÂÄº
        default_model = "gemini-2.5-pro"
        logger.warning(f"Model '{anthropic_model}' not found in mapping, falling back to default '{default_model}'")
        return default_model

    def convert_messages(self, messages: List[Message]) -> List[Dict[str, str]]:
        """ËΩ¨Êç¢Ê∂àÊÅØÊ†ºÂºè"""
        converted_messages = []
        
        for msg in messages:
            if isinstance(msg.content, str):
                converted_messages.append({
                    "role": msg.role,
                    "content": msg.content
                })
            else:
                # Â§ÑÁêÜÂ§çÊùÇÂÜÖÂÆπÂùó
                content_text = ""
                for block in msg.content:
                    if hasattr(block, 'type'):
                        if block.type == "text":
                            content_text += block.text + "\n"
                        elif block.type == "image":
                            content_text += "[Image content]\n"
                        elif block.type == "tool_use":
                            content_text += f"[Tool use: {block.name}]\n"
                        elif block.type == "tool_result":
                            content_text += f"[Tool result: {block.tool_use_id}]\n"
                
                converted_messages.append({
                    "role": msg.role,
                    "content": content_text.strip()
                })
        
        return converted_messages
    
    def convert_request(self, request: MessagesRequest) -> Dict[str, Any]:
        """ËΩ¨Êç¢ÂÆåÊï¥ÁöÑËØ∑Ê±Ç"""
        gemini_model = self.convert_model(request.model)
        
        converted = {
            "messages": self.convert_messages(request.messages),
            "model": gemini_model,
            "temperature": request.temperature,
            "stream": request.stream,
        }
        
        if request.max_tokens:
            converted["max_tokens"] = request.max_tokens
        
        # Ê≠§Êó•ÂøóÂ∑≤ÁßªËá≥ main.py ‰∏≠Ôºå‰ª•ÂåÖÂê´Êõ¥‰∏∞ÂØåÁöÑ‰ø°ÊÅØ
        # logger.info(f"üîÑ REQUEST CONVERSION: {request.model} ‚Üí {gemini_model}")
        return converted

class GeminiToAnthropicConverter:
    """Â∞Ü Gemini ÂìçÂ∫îËΩ¨Êç¢‰∏∫ Anthropic Ê†ºÂºè"""
    
    def convert_usage(self, gemini_usage) -> Usage:
        """ËΩ¨Êç¢‰ΩøÁî®ÁªüËÆ°‰ø°ÊÅØ"""
        input_tokens = 0
        output_tokens = 0
        
        if gemini_usage:
            if hasattr(gemini_usage, 'prompt_tokens'):
                input_tokens = gemini_usage.prompt_tokens
            elif hasattr(gemini_usage, 'input_tokens'):
                input_tokens = gemini_usage.input_tokens
            elif isinstance(gemini_usage, dict):
                input_tokens = gemini_usage.get('prompt_tokens', gemini_usage.get('input_tokens', 0))
                
            if hasattr(gemini_usage, 'completion_tokens'):
                output_tokens = gemini_usage.completion_tokens
            elif hasattr(gemini_usage, 'output_tokens'):
                output_tokens = gemini_usage.output_tokens
            elif isinstance(gemini_usage, dict):
                output_tokens = gemini_usage.get('completion_tokens', gemini_usage.get('output_tokens', 0))
            
        return Usage(
            input_tokens=input_tokens,
            output_tokens=output_tokens
        )
    
    def convert_content(self, gemini_content: str) -> List[ContentBlockText]:
        """ËΩ¨Êç¢ÂÜÖÂÆπÔºåÂ§ÑÁêÜÁ©∫ÂÜÖÂÆπÁöÑÊÉÖÂÜµ"""
        if not gemini_content:
            gemini_content = ""
        return [ContentBlockText(type="text", text=str(gemini_content))]
    
    def convert_response(self, gemini_response, original_request: MessagesRequest) -> MessagesResponse:
        """ËΩ¨Êç¢ÂÆåÊï¥ÂìçÂ∫îÔºåÂ¢ûÂº∫ÈîôËØØÂ§ÑÁêÜ"""
        try:
            content = ""
            usage = None
            stop_reason = "end_turn"
            
            if hasattr(gemini_response, 'choices') and gemini_response.choices:
                choice = gemini_response.choices[0]
                
                if hasattr(choice, 'message') and hasattr(choice.message, 'content'):
                    content = choice.message.content or ""
                elif hasattr(choice, 'text'):
                    content = choice.text or ""
                
                if hasattr(choice, 'finish_reason'):
                    finish_reason = choice.finish_reason
                    if finish_reason == 'length':
                        stop_reason = "max_tokens"
                    elif finish_reason in ['stop', 'end_turn']:
                        stop_reason = "end_turn"
                    elif finish_reason == 'function_call':
                        stop_reason = "tool_use"
                
                usage = getattr(gemini_response, 'usage', None)
                
            elif isinstance(gemini_response, dict):
                choices = gemini_response.get('choices', [])
                if choices:
                    choice = choices[0]
                    message = choice.get('message', {})
                    content = message.get('content', '')
                    
                    finish_reason = choice.get('finish_reason')
                    if finish_reason == 'length':
                        stop_reason = "max_tokens"
                    elif finish_reason in ['stop', 'end_turn']:
                        stop_reason = "end_turn"
                
                usage = gemini_response.get('usage')
            
            response_id = f"msg_{uuid.uuid4().hex[:24]}"
            
            return MessagesResponse(
                id=response_id,
                model=original_request.model,
                role="assistant",
                content=self.convert_content(content),
                stop_reason=stop_reason,
                usage=self.convert_usage(usage)
            )
            
        except Exception as e:
            logger.error(f"Error converting response: {e}")
            logger.error(f"Response type: {type(gemini_response)}")
            logger.error(f"Response content: {str(gemini_response)[:500]}...")
            
            return MessagesResponse(
                id=f"msg_{uuid.uuid4().hex[:24]}",
                model=original_request.model,
                role="assistant",
                content=[ContentBlockText(type="text", text=f"Error processing response: {str(e)}")],
                stop_reason="end_turn",
                usage=Usage(input_tokens=0, output_tokens=0)
            )

# ========== ÊµÅÂºèÂìçÂ∫îÁîüÊàêÂô® ==========

class StreamingResponseGenerator:
    """ÁîüÊàê Anthropic Ê†ºÂºèÁöÑÊµÅÂºèÂìçÂ∫î"""
    
    def __init__(self, original_request: MessagesRequest):
        self.original_request = original_request
        self.message_id = f"msg_{uuid.uuid4().hex[:24]}"
        self.input_tokens = 0
    
    async def generate_sse_events(self, gemini_stream) -> AsyncGenerator[str, None]:
        """ÁîüÊàê SSE ‰∫ã‰ª∂ÊµÅ"""
        try:
            self.input_tokens = self._estimate_input_tokens()
            
            yield self._create_message_start()
            yield self._create_content_block_start()
            yield self._create_ping()
            
            accumulated_text = ""
            output_tokens = 0
            chunk_count = 0
            last_ping_time = time.time()
            
            async for chunk in gemini_stream:
                chunk_count += 1
                current_time = time.time()
                
                if current_time - last_ping_time > 30:
                    yield self._create_ping()
                    last_ping_time = current_time
                
                try:
                    if hasattr(chunk, 'choices') and chunk.choices:
                        choice = chunk.choices[0]
                        if hasattr(choice, 'delta') and hasattr(choice.delta, 'content'):
                            content = choice.delta.content
                            if content:
                                accumulated_text += content
                                output_tokens += self._estimate_token_count(content)
                                yield self._create_content_block_delta(content)
                    
                    if hasattr(chunk, 'usage') and chunk.usage:
                        usage = chunk.usage
                        if hasattr(usage, 'completion_tokens'):
                            output_tokens = usage.completion_tokens
                        if hasattr(usage, 'prompt_tokens'):
                            self.input_tokens = usage.prompt_tokens
                            
                except Exception as chunk_error:
                    logger.warning(f"Error processing chunk {chunk_count}: {chunk_error}")
                    continue
            
            yield self._create_content_block_stop()
            yield self._create_message_delta(output_tokens)
            yield self._create_message_stop()
            yield "data: [DONE]\n\n"
            
            logger.info(f"Streaming completed: {chunk_count} chunks, {output_tokens} output tokens")
            
        except Exception as e:
            logger.error(f"Error in streaming response: {e}")
            yield self._create_error_event(str(e))
    
    def _create_message_start(self) -> str:
        """ÂàõÂª∫ message_start ‰∫ã‰ª∂"""
        message_data = {
            'type': 'message_start',
            'message': {
                'id': self.message_id,
                'type': 'message',
                'role': 'assistant',
                'model': self.original_request.model,
                'content': [],
                'stop_reason': None,
                'stop_sequence': None,
                'usage': {
                    'input_tokens': self.input_tokens,
                    'cache_creation_input_tokens': 0,
                    'cache_read_input_tokens': 0,
                    'output_tokens': 0
                }
            }
        }
        return f"event: message_start\ndata: {json.dumps(message_data)}\n\n"
    
    def _create_content_block_start(self) -> str:
        """ÂàõÂª∫ content_block_start ‰∫ã‰ª∂"""
        return f"event: content_block_start\ndata: {json.dumps({'type': 'content_block_start', 'index': 0, 'content_block': {'type': 'text', 'text': ''}})}\n\n"
    
    def _create_ping(self) -> str:
        """ÂàõÂª∫ ping ‰∫ã‰ª∂"""
        return f"event: ping\ndata: {json.dumps({'type': 'ping'})}\n\n"
    
    def _create_content_block_delta(self, text: str) -> str:
        """ÂàõÂª∫ content_block_delta ‰∫ã‰ª∂"""
        return f"event: content_block_delta\ndata: {json.dumps({'type': 'content_block_delta', 'index': 0, 'delta': {'type': 'text_delta', 'text': text}})}\n\n"
    
    def _create_message_delta(self, output_tokens: int) -> str:
        """ÂàõÂª∫ message_delta ‰∫ã‰ª∂"""
        return f"event: message_delta\ndata: {json.dumps({'type': 'message_delta', 'delta': {'stop_reason': 'end_turn', 'stop_sequence': None}, 'usage': {'output_tokens': output_tokens}})}\n\n"
    
    def _create_message_stop(self) -> str:
        """ÂàõÂª∫ message_stop ‰∫ã‰ª∂"""
        return f"event: message_stop\ndata: {json.dumps({'type': 'message_stop'})}\n\n"
    
    def _estimate_input_tokens(self) -> int:
        """‰º∞ÁÆóËæìÂÖ•tokenÊï∞Èáè"""
        total_chars = 0
        for message in self.original_request.messages:
            if isinstance(message.content, str):
                total_chars += len(message.content)
            elif isinstance(message.content, list):
                for block in message.content:
                    if hasattr(block, 'text'):
                        total_chars += len(block.text)
        
        return max(1, total_chars // 4)
    
    def _estimate_token_count(self, text: str) -> int:
        """‰º∞ÁÆóÊñáÊú¨ÁöÑtokenÊï∞Èáè"""
        return max(1, len(text.split()) // 0.75)
    
    def _create_content_block_stop(self) -> str:
        """ÂàõÂª∫ content_block_stop ‰∫ã‰ª∂"""
        return f"event: content_block_stop\ndata: {json.dumps({'type': 'content_block_stop', 'index': 0})}\n\n"
    
    def _create_error_event(self, error_message: str) -> str:
        """ÂàõÂª∫ÈîôËØØ‰∫ã‰ª∂"""
        error_data = {
            'type': 'error',
            'error': {
                'type': 'internal_server_error',
                'message': error_message
            }
        }
        return f"event: error\ndata: {json.dumps(error_data)}\n\n"

# ========== Â∑•ÂÖ∑ÊîØÊåÅ ==========

class ToolConverter:
    """Â∑•ÂÖ∑Ë∞ÉÁî®ËΩ¨Êç¢Âô®ÔºåÂ¢ûÂº∫ÂäüËÉΩ"""
    
    def convert_tools_to_gemini(self, tools: List[Tool]) -> List[Dict[str, Any]]:
        """Â∞Ü Anthropic Â∑•ÂÖ∑ËΩ¨Êç¢‰∏∫ Gemini Ê†ºÂºè"""
        gemini_tools = []
        
        for tool in tools:
            try:
                schema = tool.input_schema
                if not isinstance(schema, dict):
                    logger.warning(f"Invalid schema for tool {tool.name}, skipping")
                    continue
                
                if 'required' not in schema and 'properties' in schema:
                    schema['required'] = []
                
                gemini_tool = {
                    "function_declarations": [{
                        "name": tool.name,
                        "description": tool.description or f"Execute {tool.name} function",
                        "parameters": schema
                    }]
                }
                gemini_tools.append(gemini_tool)
                logger.debug(f"Converted tool: {tool.name}")
                
            except Exception as e:
                logger.error(f"Error converting tool {tool.name}: {e}")
                continue
        
        return gemini_tools
    
    def convert_tool_choice_to_gemini(self, tool_choice: Dict[str, Any]) -> str:
        """ËΩ¨Êç¢Â∑•ÂÖ∑ÈÄâÊã©ÂèÇÊï∞ÔºåÂ¢ûÂº∫ÈîôËØØÂ§ÑÁêÜ"""
        if not isinstance(tool_choice, dict):
            logger.warning(f"Invalid tool_choice format: {type(tool_choice)}")
            return "AUTO"
        
        choice_type = tool_choice.get("type", "auto").lower()
        
        if choice_type == "auto":
            return "AUTO"
        elif choice_type == "any":
            return "ANY"
        elif choice_type == "tool" and "name" in tool_choice:
            return tool_choice["name"]
        elif choice_type == "none":
            return "NONE"
        else:
            logger.warning(f"Unknown tool_choice type: {choice_type}")
            return "AUTO"
    
    def convert_gemini_tool_calls_to_anthropic(self, gemini_calls) -> List[ContentBlockToolUse]:
        """Â∞ÜGeminiÁöÑÂ∑•ÂÖ∑Ë∞ÉÁî®ËΩ¨Êç¢‰∏∫AnthropicÊ†ºÂºè"""
        anthropic_calls = []
        
        for call in gemini_calls:
            try:
                tool_use = ContentBlockToolUse(
                    type="tool_use",
                    id=f"toolu_{uuid.uuid4().hex[:24]}",
                    name=call.get("name", "unknown_tool"),
                    input=call.get("parameters", {})
                )
                anthropic_calls.append(tool_use)
            except Exception as e:
                logger.error(f"Error converting tool call: {e}")
                continue
        
        return anthropic_calls
